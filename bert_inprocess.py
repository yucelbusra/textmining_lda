# -*- coding: utf-8 -*-
"""Copy of BERT_inprocess.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1v48nz-Om-Sbi0gs1apq2Rwk_ZNz29-4N
"""

from google.colab import drive
drive.mount('/content/drive')
import pandas as pd

import torch
print("CUDA available:", torch.cuda.is_available())
print("Device name:", torch.cuda.get_device_name(0) if torch.cuda.is_available() else "CPU only")

import json

def excel_to_json(excel_file, json_file):
    # Read the Excel file into a Pandas DataFrame
    df = pd.read_excel('/content/drive/MyDrive/Colab Notebooks/Data/file segmentationv2.xlsx')

    # Convert the DataFrame to a list of dictionaries (records)
    data = df.to_dict(orient="records")

    # Write the JSON data to a file
    with open(json_file, "w", encoding="utf-8") as f:
        json.dump(data, f, indent=4)

    print(f"Data successfully converted from {excel_file} to {json_file}")

excel_to_json("excel_file", "text_deneme.json")

# Load JSON file
with open("/content/text_deneme.json", "r", encoding="utf-8") as f:
    data = json.load(f)  # Load the JSON file as a list of dictionaries

# Extract all "Unnamed: 2" values
docs = [record["Unnamed: 2"] for record in data if "Unnamed: 2" in record]

# Check the first few extracted texts
print(docs[:3])

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install bertopic
#

from bertopic import BERTopic
import json

import spacy
import re

# Load English NLP model
nlp = spacy.load("en_core_web_sm")

# Define additional stopwords
custom_stopwords = {"construction", "technology", "industry","use", "case", "sector" "project", "practical", "nerd", "practical nerd", "crunch", "document crunch", "suolk", "suffolk", "john deere", "mind", "mind ai", "windover", "cemex", "ryan", "john", "deere", "like"}

# Function to normalize specific terms to their abbreviated forms
def normalize_text(text):
    text = text.lower()
    text = text.replace('artificial intelligence', 'ai')
    text = text.replace('construction technology', 'contech')
    text = text.replace('machine learning', 'ml')
    text = text.replace('supply chain', 'supplychain')
    text = text.replace('merger and acquisition', 'M&A')
    text = text.replace('software-as-a-service', 'SaaS')
    text = text.replace('software as a service', 'SaaS')
    text = text.replace('outcome as a service', 'OaaS')
    text = text.replace('outcome-as-a-service', 'OaaS')
    return text

# Function to clean and preprocess text
def preprocess_text(text):
    text = normalize_text(text)  # Normalize terms
    text = re.sub(r'\s+', ' ', text)  # Remove extra spaces
    text = re.sub(r'[^a-zA-Z\s]', '', text)  # Remove special characters
    doc = nlp(text)
    tokens = [token.lemma_ for token in doc if not token.is_stop and token.is_alpha and token.lemma_ not in custom_stopwords]  # Lemmatize & remove stopwords
    return " ".join(tokens)

# Preprocess all documents
clean_docs = [preprocess_text(doc) for doc in docs]

from umap import UMAP
from hdbscan import HDBSCAN
from sentence_transformers import SentenceTransformer
from sklearn.feature_extraction.text import CountVectorizer

from bertopic import BERTopic
from bertopic.representation import KeyBERTInspired
from bertopic.vectorizers import ClassTfidfTransformer


# Step 1 - Extract embeddings
embedding_model = SentenceTransformer("all-MiniLM-L6-v2")
embeddings = embedding_model.encode(clean_docs, show_progress_bar=True)
# Step 2 - Reduce dimensionality
umap_model = UMAP(n_neighbors=15, n_components=5, min_dist=0.0, metric='cosine', random_state=42)

# Step 3 - Cluster reduced embeddings
hdbscan_model = HDBSCAN(min_cluster_size=2, metric='euclidean', cluster_selection_method='eom', prediction_data=True)

# Step 4 - Tokenize topics
from sklearn.feature_extraction.text import CountVectorizer
vectorizer_model = CountVectorizer(stop_words="english", min_df=2, ngram_range=(2, 2))

import openai
from bertopic.representation import KeyBERTInspired, MaximalMarginalRelevance, OpenAI, PartOfSpeech

# KeyBERT
keybert_model = KeyBERTInspired()

# Part-of-Speech
pos_model = PartOfSpeech("en_core_web_sm")

# MMR
mmr_model = MaximalMarginalRelevance(diversity=0.3)

# GPT-3.5
prompt = """
I have a topic that contains the following documents:
[DOCUMENTS]
The topic is described by the following keywords: [KEYWORDS]

Based on the information above, extract a short but highly descriptive topic label of at most 5 words. Make sure it is in the following format:
topic: <topic label>
"""
client = openai.OpenAI(api_key="sk-proj.....")
openai_model = OpenAI(client, model="gpt-4o-mini", exponential_backoff=True, chat=True, prompt=prompt, verbose=True)


# All representation models
representation_model = {
    "KeyBERT": keybert_model,
    "OpenAI": openai_model,  # Uncomment if you will use OpenAI
    "MMR": mmr_model,
    "POS": pos_model
}

from bertopic import BERTopic

topic_model = BERTopic(

  # Pipeline models
  embedding_model=embedding_model,
  umap_model=umap_model,
  hdbscan_model=hdbscan_model,
  vectorizer_model=vectorizer_model,
  representation_model=representation_model,

  # Hyperparameters
  top_n_words=15,
  verbose=True
)

# Step 1: Fit the topic model
#topics, probs = topic_model.fit_transform(clean_docs)

# Step 2: Reduce the number of topics AFTER training
#topic_model.reduce_topics(clean_docs, nr_topics=18)


topics, probs = topic_model.fit_transform(clean_docs, embeddings)

chatgpt_topic_labels = {topic: " | ".join(list(zip(*values))[0]) for topic, values in topic_model.topic_aspects_["OpenAI"].items()}
chatgpt_topic_labels[-1] = "Outlier Topic"
topic_model.set_topic_labels(chatgpt_topic_labels)

topic_model.get_topic_info()

import pandas as pd

# Get topic info for labeling
topic_info = topic_model.get_topic_info()
topic_labels = topic_info.set_index("Topic")["Name"].to_dict()

# Create DataFrame with results
df = pd.DataFrame({
    "Document": clean_docs,
    "Topic": topics,
    "Topic_Label": [topic_labels.get(t, "Unknown") for t in topics],
    "Probability": probs  # optional
})

# Save to CSV
df.to_csv("bertopic_output.csv", index=False)
print("âœ… Saved to 'bertopic_output.csv'")

topic_model.get_topic(1, full=True)

topic_model.get_topic(0, full=True)

topic_model.get_topic(2, full=True)

topic_model.get_topic(3, full=True)

# `topic_distr` contains the distribution of topics in each document
topic_distr, _ = topic_model.approximate_distribution(clean_docs, window=8, stride=4)

topic_model.visualize_distribution(topic_distr[1])

from wordcloud import WordCloud
import matplotlib.pyplot as plt

def create_wordcloud(model, topic):
    text = {word: value for word, value in model.get_topic(topic)}
    wc = WordCloud(background_color="white", max_words=1000)
    wc.generate_from_frequencies(text)
    plt.imshow(wc, interpolation="bilinear")
    plt.axis("off")
    plt.show()

# Show wordcloud
create_wordcloud(topic_model, topic=0)

from wordcloud import WordCloud
import matplotlib.pyplot as plt

def create_wordcloud(model, topic):
    text = {word: value for word, value in model.get_topic(topic)}
    wc = WordCloud(background_color="white", max_words=1000)
    wc.generate_from_frequencies(text)
    plt.imshow(wc, interpolation="bilinear")
    plt.axis("off")
    plt.show()

# Show wordcloud
create_wordcloud(topic_model, topic=1)

from wordcloud import WordCloud
import matplotlib.pyplot as plt

def create_wordcloud(model, topic):
    text = {word: value for word, value in model.get_topic(topic)}
    wc = WordCloud(background_color="white", max_words=1000)
    wc.generate_from_frequencies(text)
    plt.imshow(wc, interpolation="bilinear")
    plt.axis("off")
    plt.show()

# Show wordcloud
create_wordcloud(topic_model, topic=2)

from wordcloud import WordCloud
import matplotlib.pyplot as plt

def create_wordcloud(model, topic):
    text = {word: value for word, value in model.get_topic(topic)}
    wc = WordCloud(background_color="white", max_words=1000)
    wc.generate_from_frequencies(text)
    plt.imshow(wc, interpolation="bilinear")
    plt.axis("off")
    plt.show()

# Show wordcloud
create_wordcloud(topic_model, topic=3)

from wordcloud import WordCloud
import matplotlib.pyplot as plt

def create_wordcloud(model, topic):
    text = {word: value for word, value in model.get_topic(topic)}
    wc = WordCloud(background_color="white", max_words=1000)
    wc.generate_from_frequencies(text)
    plt.imshow(wc, interpolation="bilinear")
    plt.axis("off")
    plt.show()

# Show wordcloud
create_wordcloud(topic_model, topic=4)

from wordcloud import WordCloud
import matplotlib.pyplot as plt

def create_wordcloud(model, topic):
    text = {word: value for word, value in model.get_topic(topic)}
    wc = WordCloud(background_color="white", max_words=1000)
    wc.generate_from_frequencies(text)
    plt.imshow(wc, interpolation="bilinear")
    plt.axis("off")
    plt.show()

# Show wordcloud
create_wordcloud(topic_model, topic=5)

from wordcloud import WordCloud
import matplotlib.pyplot as plt

def create_wordcloud(model, topic):
    text = {word: value for word, value in model.get_topic(topic)}
    wc = WordCloud(background_color="white", max_words=1000)
    wc.generate_from_frequencies(text)
    plt.imshow(wc, interpolation="bilinear")
    plt.axis("off")
    plt.show()

# Show wordcloud
create_wordcloud(topic_model, topic=6)

from wordcloud import WordCloud
import matplotlib.pyplot as plt

def create_wordcloud(model, topic):
    text = {word: value for word, value in model.get_topic(topic)}
    wc = WordCloud(background_color="white", max_words=1000)
    wc.generate_from_frequencies(text)
    plt.imshow(wc, interpolation="bilinear")
    plt.axis("off")
    plt.show()

# Show wordcloud
create_wordcloud(topic_model, topic=7)

from wordcloud import WordCloud
import matplotlib.pyplot as plt

def create_wordcloud(model, topic):
    text = {word: value for word, value in model.get_topic(topic)}
    wc = WordCloud(background_color="white", max_words=1000)
    wc.generate_from_frequencies(text)
    plt.imshow(wc, interpolation="bilinear")
    plt.axis("off")
    plt.show()

# Show wordcloud
create_wordcloud(topic_model, topic=8)

from wordcloud import WordCloud
import matplotlib.pyplot as plt

def create_wordcloud(model, topic):
    text = {word: value for word, value in model.get_topic(topic)}
    wc = WordCloud(background_color="white", max_words=1000)
    wc.generate_from_frequencies(text)
    plt.imshow(wc, interpolation="bilinear")
    plt.axis("off")
    plt.show()

# Show wordcloud
create_wordcloud(topic_model, topic=9)

from wordcloud import WordCloud
import matplotlib.pyplot as plt

def create_wordcloud(model, topic):
    text = {word: value for word, value in model.get_topic(topic)}
    wc = WordCloud(background_color="white", max_words=1000)
    wc.generate_from_frequencies(text)
    plt.imshow(wc, interpolation="bilinear")
    plt.axis("off")
    plt.show()

# Show wordcloud
create_wordcloud(topic_model, topic=10)

# Calculate the topic distributions on a token-level
topic_distr, topic_token_distr = topic_model.approximate_distribution(clean_docs[2], calculate_tokens=True)

# Visualize the token-level distributions
df = topic_model.visualize_approximate_distribution(clean_docs[2], topic_token_distr[0])
df

topic_model.visualize_hierarchy(custom_labels= True)

reduced_embeddings = UMAP(n_neighbors=10, n_components=2, min_dist=0.0, metric='cosine').fit_transform(embeddings)

topic_model.visualize_documents(clean_docs, reduced_embeddings=reduced_embeddings, custom_labels= True)

topic_model.visualize_documents(clean_docs, custom_labels= True)

fig = topic_model.visualize_documents(
    clean_docs,
    hide_annotations=True,
    custom_labels= True
)

# Increase marker size
fig.update_traces(marker=dict(size=10))  # Try 12, 15, etc. for bigger nodes

# Show the modified figure
fig.show()

topic_model.visualize_topics()

topic_model.visualize_barchart()

topic_model.visualize_heatmap()

topic_model.visualize_term_rank()